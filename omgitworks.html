<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning</title>
<!--Generated on Thu Dec 28 14:55:51 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="reinforcement learning,  autonomous,  discriminator">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Archit Sharma
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rehaan Ahmad
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chelsea Finn
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">While reinforcement learning (RL) provides a framework for learning through trial and error, translating RL algorithms into the real world has remained challenging. A major hurdle to real-world application arises from the development of algorithms in an episodic setting where the environment is reset after every trial, in contrast with the continual and non-episodic nature of the real-world encountered by embodied agents such as humans and robots.
Enabling agents to learn behaviors autonomously in such non-episodic environments requires that the agent to be able to conduct its own trials.
Prior works have considered an alternating approach where a forward policy learns to solve the task and the backward policy learns to reset the environment, but what initial state distribution should the backward policy reset the agent to? Assuming access to a few demonstrations, we propose a new method, MEDAL, that trains the backward policy to match the state distribution in the provided demonstrations. This keeps the agent close to the task-relevant states, allowing for a mix of easy and difficult starting states for the forward policy. Our experiments show that MEDAL matches or outperforms prior methods on three sparse-reward continuous control tasks from the EARL benchmark, with 40% gains on the hardest task, while making fewer assumptions than prior works. Code and videos are at: <span class="ltx_ref ltx_ref_self">https://sites.google.com/view/medal-arl/home</span></p>
</div>
<div class="ltx_keywords">reinforcement learning, autonomous, discriminator
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">A cornerstone of human and animal intelligence is the ability to learn autonomously through trial and error. To that extent, reinforcement learning (RL) presents a natural framework to develop learning algorithms for embodied agents. Unfortunately, the predominant emphasis on episodic learning represents a departure from the continual non-episodic nature of the real-world, which presents multiple technical challenges. First, episodic training undermines the autonomy of the learning agent by requiring repeated extrinsic interventions to reset the environment after every trial, which can be both time-consuming and expensive as these interventions may have to be conducted by a human. Second, episodic training from narrow initial state distributions can lead to less robust policies that are reliant on environment resets to recover; e.g. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomous</span></cite> show that policies learned in episodic settings with narrow initial state distributions are more sensitive to perturbations than those trained in non-episodic settings.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="figures/medal.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of our proposed method MEDAL (<span class="ltx_text ltx_font_italic">right</span>) contrasting it with forward-backward RL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">han2015learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">eysenbach2017leave</span>)</cite> (<span class="ltx_text ltx_font_italic">left</span>). MEDAL trains a backward policy <math id="S1.F1.m4" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> to pull the agent back to the state distribution defined by the demonstrations, enabling the forward policy <math id="S1.F1.m5" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math> to the learn the task efficiently in contrast to FBRL that retrieves the agent to the initial state distribution before every trial of <math id="S1.F1.m6" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math>.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Prior works have found that conventional RL algorithms substantially depreciate in performance when applied in non-episodic settings <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">co2020ecological</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu20ingredients</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomous</span>)</cite>.
Why do such algorithms struggle to learn in non-episodic, autonomous RL (ARL) settings?
Resetting the environment after every single episode allows for natural repetition: the agent can repeatedly practice the task under a narrow set of initial conditions to incrementally improve the policy. Critically, algorithms developed for episodic learning do not have to learn how to reach these initial conditions in the first place. Thus, the main additional challenge in non-episodic, autonomous RL settings is to enable the repetitive practice that is necessary to learn an adept policy. For example, an autonomous robot that is practicing how to close a door will also need to learn how to open a door.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Several recent works learn a backward policy to enable the main forward policy to practice the task: for example, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">han2015learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">eysenbach2017leave</span></cite> propose a backward policy that learns to match the initial state distribution.
However, unlike the episodic setting, the agent can practice the task from any initial state, and not just the narrow initial state distribution that is usually provided by resets. Can the backward policy create starting conditions that enable the forward policy to improve efficiently? It could be useful for the agent to try the task both from “easy” states that are close to the goal and harder states that are representative of the starting conditions at evaluation. Easier and harder initial conditions can be seen as a curriculum that simplifies exploration.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">kakade2002approximately</span></cite> provide a theoretical discussion on how the initial state distribution affects the performance of the learned policy. One of the results show that the closer the starting state distribution is to the <span class="ltx_text ltx_font_italic">state distribution of the optimal policy</span> <math id="S1.p3.m1" class="ltx_Math" alttext="\rho^{*}" display="inline"><msup><mi>ρ</mi><mo>*</mo></msup></math>, the faster the policy moves toward the optimal policy <math id="S1.p3.m2" class="ltx_Math" alttext="\pi^{*}" display="inline"><msup><mi>π</mi><mo>*</mo></msup></math>.
While an oracle access to <math id="S1.p3.m3" class="ltx_Math" alttext="\rho^{*}" display="inline"><msup><mi>ρ</mi><mo>*</mo></msup></math> is rarely available, we often have access to a modest set of demonstrations. In this work, we aim to improve autonomous RL by learning a backward policy that matches the starting state distribution to the state distribution observed in the demonstrations.
This enables the agent to practice the task from a variety of initial states, including some that are possibly easier to explore from. An intuitive representation of the algorithm is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">The primary contribution of our work is an autonomous RL algorithm <span class="ltx_text ltx_font_italic">Matching Expert Distributions for Autonomous Learning</span> (MEDAL), which learns a backward policy that matches the state distribution of a small set of demonstrations, in conjunction with a forward policy that optimizes the task reward. We use a classification based approach that implicitly minimizes the distance between the state distribution of the backward policy and the state distribution in the demonstrations without requiring the density under either distribution.
In Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:experiments</span>, we empirically analyze the performance of MEDAL on the Environments for Autonomous RL (EARL) benchmark <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomous</span>)</cite>. We find that MEDAL matches or outperforms competitive baselines in all of the sparse-reward environments, with a more than a 40% gain in success rate on the hardest task where all other comparisons fail completely. Our ablations additionally indicate the importance of matching the state distribution in the demonstrations, providing additional empirical support for the hypothesis that the expert state distribution constitutes a good starting state distribution for learning a task.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Autonomous RL.</span> Using additional policies to enable autonomous learning goes back to the works of <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">rivest1993inference</span>)</cite> in context of finite state automaton, also referred to as “homing strategies” in <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">even2005reinforcement</span>)</cite> in context of POMDPs. More recently, in context of continuous control, several works propose autonomous RL methods targeting different starting distributions to learn from: <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">han2015learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">eysenbach2017leave</span></cite> match the initial state distribution, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu20ingredients</span></cite> leverage state-novelty <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">burda2018exploration</span>)</cite> to create new starting conditions for every trial, and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomouscurr</span></cite> create a curriculum of starting states based on the performance of the forward policy to accelerate the learning. In addition, <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2020continual</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">lu2020reset</span>)</cite> leverage ideas from unsupervised skill discovery <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gregor2016variational</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">eysenbach2018diversity</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2019dynamics</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">hazan2019provably</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">campos2020explore</span>)</cite>,
with the former using it to create an adversarial initial state distribution and the latter to tackle non-episodic lifelong learning with a non-stationary task-distribution. Our work proposes a novel algorithm MEDAL that, unlike these prior works,
opts to match the starting distribution to the state distribution
of demonstrations.
Value-accelerated Persistent RL (VaPRL) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomouscurr</span>)</cite> also considers the problem of autonomous RL with a few initial demonstrations.
Unlike VaPRL, our algorithm does not rely on relabeling transitions with new goals <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">andrychowicz2017hindsight</span>)</cite>, and thus does not require access to the functional form of the reward function, eliminating the need for additional hyperparameters that require task-specific tuning.
A simple and task-agnostic ARL method would accelerate the development of autonomous robotic systems, the benefits of such autonomy being demonstrated by several recent works
<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chatzilygeroudis2018reset</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gupta2021reset</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">smith2021legged</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ha2020learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">bloesch2022towards</span>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Distribution Matching in RL.</span> Critical to our method is matching the state distribution of the demonstrations.
Such a distribution matching perspective is often employed in inverse RL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ng2000algorithms</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ziebart2008maximum</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ziebart2010modeling</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">finn2016guided</span>)</cite> and imitation learning <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ghasemipour2020divergence</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">argall2009survey</span>)</cite> or to encourage efficient exploration <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">lee2019efficient</span>)</cite>. More recently, several works have leveraged implicit distribution matching by posing a classification problem, pioneered in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">goodfellow2014generative</span></cite>, to imitate demonstrations <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ho2016generative</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">baram2017end</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">kostrikov2018discriminator</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">rafailov2021visual</span>)</cite>, to imitate sequences of observations <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">torabi2019adversarial</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu2020off</span>)</cite>, or to learn reward functions for goal-reaching <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">fu2018variational</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">singh2019end</span>)</cite>. Our work employs a similar discriminator-based approach to encourage the state distribution induced by the policy to match that of the demonstrations. Importantly, our work focuses on creating an initial state distribution that the forward policy can learn efficiently from, as opposed to these prior works that are designed for the episodic RL setting. As the experiments in Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:gail</span> and Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:init_state_ablation</span> show, naïve extensions of these methods to non-episodic settings don’t fare well.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Accelerating RL using Demonstrations.</span> There is rich literature on using demonstrations to speed up reinforcement learning, especially for sparse reward problems.
Prior works have considering shaping rewards using demonstrations <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">brys2015reinforcement</span>)</cite>, pre-training the policy <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">rajeswaran2017learning</span>)</cite>, using behavior cloning loss as a regularizer for policy gradients <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">rajeswaran2017learning</span>)</cite> and <math id="S2.p3.m1" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math>-learning <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">nair2018overcoming</span>)</cite>, and initializing the replay buffer <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">nair2018overcoming</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">vecerik2017leveraging</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">hester2018deep</span>)</cite>. MEDAL leverages demonstrations to accelerate non-episodic reinforcement learning by utilizing demo distribution to create initial conditions for the forward policy. The techniques proposed in these prior works are complimentary to our proposal, and can be leveraged for non-episodic RL in general as well. Indeed, for all methods in our experiments, the replay buffer is initialized with demonstrations.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Autonomous Reinforcement Learning</span>. We use the ARL framework for non-episodic learning defined in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomous</span></cite>, which we briefly summarize here.
Consider a Markov decision process <math id="S3.p1.m1" class="ltx_Math" alttext="{\mathcal{M}\equiv(\mathcal{S},\mathcal{A},p,r,\rho_{0})}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo>≡</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>r</mi><mo>,</mo><msub><mi>ρ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow></math>, where <math id="S3.p1.m2" class="ltx_Math" alttext="\mathcal{S}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒮</mi></math> denotes the state space, <math id="S3.p1.m3" class="ltx_Math" alttext="\mathcal{A}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒜</mi></math> denotes the action space, <math id="S3.p1.m4" class="ltx_Math" alttext="{p:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\mapsto\mathbb{R}_{\geq 0}}" display="inline"><mrow><mi>p</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi class="ltx_font_mathcaligraphic">𝒮</mi></mrow><mo stretchy="false">↦</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow></math> denotes the transition dynamics, <math id="S3.p1.m5" class="ltx_Math" alttext="{r:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}}" display="inline"><mrow><mi>r</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi></mrow><mo stretchy="false">↦</mo><mi>ℝ</mi></mrow></mrow></math> denotes the reward function and <math id="S3.p1.m6" class="ltx_Math" alttext="\rho_{0}" display="inline"><msub><mi>ρ</mi><mn>0</mn></msub></math> denotes the initial state distribution. The learning algorithm <math id="S3.p1.m7" class="ltx_Math" alttext="\mathbb{A}" display="inline"><mi>𝔸</mi></math> is defined as <math id="S3.p1.m8" class="ltx_Math" alttext="{\mathbb{A}:\{s_{i},a_{i},s_{i+1},r_{i}\}_{i=0}^{t}\mapsto\{a_{t},\pi_{t}\}}" display="inline"><mrow><mi>𝔸</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></msubsup><mo stretchy="false">↦</mo><mrow><mo stretchy="false">{</mo><msub><mi>a</mi><mi>t</mi></msub><mo>,</mo><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">}</mo></mrow></mrow></mrow></math>, which maps the transitions collected in the environment until time <math id="S3.p1.m9" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> to an action <math id="S3.p1.m10" class="ltx_Math" alttext="a_{t}" display="inline"><msub><mi>a</mi><mi>t</mi></msub></math> and its best guess at the optimal policy <math id="S3.p1.m11" class="ltx_Math" alttext="{\pi_{t}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}_{\geq 0}}" display="inline"><mrow><msub><mi>π</mi><mi>t</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi></mrow><mo stretchy="false">↦</mo><msub><mi>ℝ</mi><mrow><mi></mi><mo>≥</mo><mn>0</mn></mrow></msub></mrow></mrow></math>. First, the initial state is sampled exactly once (<math id="S3.p1.m12" class="ltx_Math" alttext="s_{0}\sim\rho_{0}" display="inline"><mrow><msub><mi>s</mi><mn>0</mn></msub><mo>∼</mo><msub><mi>ρ</mi><mn>0</mn></msub></mrow></math>) at the beginning of the interaction and the learning algorithm interacts with the environment through the actions <math id="S3.p1.m13" class="ltx_Math" alttext="a_{t}" display="inline"><msub><mi>a</mi><mi>t</mi></msub></math> till <math id="S3.p1.m14" class="ltx_Math" alttext="t\to\infty" display="inline"><mrow><mi>t</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></math>. This is the key distinction from an episodic RL setting where the environment resets to a state from the initial state distribution after a few steps. Second, the action taken in the environment does not necessarily come from <math id="S3.p1.m15" class="ltx_Math" alttext="\pi_{t}" display="inline"><msub><mi>π</mi><mi>t</mi></msub></math>, for example, a backward policy <math id="S3.p1.m16" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> may generate the action taken in the environment.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">ARL defines two metrics: <span class="ltx_text ltx_font_italic">Continuing Policy Evaluation</span> measures the reward accumulated by <math id="S3.p2.m1" class="ltx_Math" alttext="\mathbb{A}" display="inline"><mi>𝔸</mi></math> over the course of training, defined as <math id="S3.p2.m2" class="ltx_Math" alttext="{\mathbb{C}(\mathbb{A})=\lim_{h\to\infty}\frac{1}{h}\mathbb{E}\left[\sum_{t=0}%
^{h}r(s_{t},a_{t})\right]}" display="inline"><mrow><mrow><mi>ℂ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝔸</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.1389em">=</mo><mrow><msub><mo lspace="0.1389em" rspace="0.167em">lim</mo><mrow><mi>h</mi><mo stretchy="false">→</mo><mi mathvariant="normal">∞</mi></mrow></msub><mrow><mfrac><mn>1</mn><mi>h</mi></mfrac><mo>⁢</mo><mi>𝔼</mi><mo>⁢</mo><mrow><mo>[</mo><mrow><msubsup><mo lspace="0em">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>h</mi></msubsup><mrow><mi>r</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow></math> and
<span class="ltx_text ltx_font_italic">Deployed Policy Evaluation</span> metric measures how quickly an algorithm improves the output policy <math id="S3.p2.m3" class="ltx_Math" alttext="\pi_{t}" display="inline"><msub><mi>π</mi><mi>t</mi></msub></math> at the task defined by the reward function <math id="S3.p2.m4" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math>, defined as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1" class="ltx_Math" alttext="\mathbb{D}(\mathbb{A})=\sum_{t=0}^{\infty}J(\pi^{*})-J(\pi_{t})," display="block"><mrow><mrow><mrow><mi>𝔻</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝔸</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">∞</mi></munderover><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>π</mi><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>−</mo><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.p2.m5" class="ltx_math_unparsed" alttext="{J(\pi)=\mathbb{E}\left[\sum_{j=0}^{\infty}\gamma^{j}r(s_{j},a_{j})\right],s_{%
0}\sim\rho_{0},a_{t}\sim\pi(\cdot\mid s_{t})}" display="inline"><mrow><mi>J</mi><mrow><mo stretchy="false">(</mo><mi>π</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mi>𝔼</mi><mrow><mo>[</mo><msubsup><mo lspace="0em">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">∞</mi></msubsup><msup><mi>γ</mi><mi>j</mi></msup><mi>r</mi><mrow><mo stretchy="false">(</mo><msub><mi>s</mi><mi>j</mi></msub><mo>,</mo><msub><mi>a</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mo>,</mo><msub><mi>s</mi><mn>0</mn></msub><mo>∼</mo><msub><mi>ρ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo>∼</mo><mi>π</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math>, <math id="S3.p2.m6" class="ltx_math_unparsed" alttext="{s_{t+1}\sim p(\cdot\mid s_{t},a_{t})}" display="inline"><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∼</mo><mi>p</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math> and <math id="S3.p2.m7" class="ltx_Math" alttext="{\pi^{*}\in\operatorname*{arg\,max}_{\pi}J(\pi)}" display="inline"><mrow><msup><mi>π</mi><mo>*</mo></msup><mo>∈</mo><mrow><mrow><msub><mrow><mi>arg</mi><mo lspace="0.170em">⁢</mo><mi>max</mi></mrow><mi>π</mi></msub><mo>⁡</mo><mi>J</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>π</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>. The goal for an algorithm <math id="S3.p2.m8" class="ltx_Math" alttext="\mathbb{A}" display="inline"><mi>𝔸</mi></math> is to minimize <math id="S3.p2.m9" class="ltx_Math" alttext="\mathbb{D}(\mathbb{A})" display="inline"><mrow><mi>𝔻</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝔸</mi><mo stretchy="false">)</mo></mrow></mrow></math>, that is to bring <math id="S3.p2.m10" class="ltx_Math" alttext="J(\pi_{t})" display="inline"><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math> close to <math id="S3.p2.m11" class="ltx_Math" alttext="J(\pi^{*})" display="inline"><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mi>π</mi><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math> in the least number of samples possible. Intuitively, minimizing <math id="S3.p2.m12" class="ltx_Math" alttext="\mathbb{D}(\mathbb{A})" display="inline"><mrow><mi>𝔻</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝔸</mi><mo stretchy="false">)</mo></mrow></mrow></math> corresponds to maximizing the area under the curve for <math id="S3.p2.m13" class="ltx_Math" alttext="J(\pi_{t})" display="inline"><mrow><mi>J</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></math> versus <math id="S3.p2.m14" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p"><math id="S3.p3.m1" class="ltx_Math" alttext="\mathbb{C}(\mathbb{A})" display="inline"><mrow><mi>ℂ</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝔸</mi><mo stretchy="false">)</mo></mrow></mrow></math> corresponds to the more conventional average-reward reinforcement learning. While algorithms are able to accumulate large rewards during training, they do not necessarily recover the optimal policy in non-episodic settings <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu20ingredients</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">co2020ecological</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomous</span>)</cite>. In response, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomous</span></cite> introduce <math id="S3.p3.m2" class="ltx_Math" alttext="\mathbb{D}(\mathbb{A})" display="inline"><mrow><mi>𝔻</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>𝔸</mi><mo stretchy="false">)</mo></mrow></mrow></math> to explicitly encourage algorithms to learn task-solving behaviors and not just accumulate reward through training.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Imitation Learning via Distribution Matching.</span> Generative Adversarial Networks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">goodfellow2016nips</span>)</cite> pioneered implicit distribution matching for distributions where likelihood cannot be computed explicitly. Given a dataset of samples <math id="S3.p4.m1" class="ltx_Math" alttext="\{x_{i}\}_{i=1}^{N}" display="inline"><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></math>, where <math id="S3.p4.m2" class="ltx_Math" alttext="x_{i}\sim p^{*}(\cdot)" display="inline"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∼</mo><mrow><msup><mi>p</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow></mrow></math> for some target distribution <math id="S3.p4.m3" class="ltx_Math" alttext="p^{*}" display="inline"><msup><mi>p</mi><mo>*</mo></msup></math> over the data space <math id="S3.p4.m4" class="ltx_Math" alttext="\mathcal{X}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒳</mi></math>, generative distribution <math id="S3.p4.m5" class="ltx_Math" alttext="p_{\theta}(\cdot)" display="inline"><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow></math> can be learned through the following minimax optimization:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1" class="ltx_Math" alttext="\min_{p_{\theta}}\max_{D}\mathbb{E}_{x\sim p^{*}}\left[\log D(x)\right]+%
\mathbb{E}_{x\sim p_{\theta}}\left[\log(1-D(x))\right]" display="block"><mrow><mrow><mrow><munder><mi>min</mi><msub><mi>p</mi><mi>θ</mi></msub></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mi>D</mi></munder><mo lspace="0.167em">⁡</mo><msub><mi>𝔼</mi><mrow><mi>x</mi><mo>∼</mo><msup><mi>p</mi><mo>*</mo></msup></mrow></msub></mrow></mrow><mo>⁢</mo><mrow><mo>[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>D</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>𝔼</mi><mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi><mi>θ</mi></msub></mrow></msub><mo>⁢</mo><mrow><mo>[</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>D</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S3.p4.m6" class="ltx_Math" alttext="D:\mathcal{X}\mapsto[0,1]" display="inline"><mrow><mi>D</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo stretchy="false">↦</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></mrow></math> is discriminator solving a binary classification problem. This can be shown to minimize the Jensen-Shannon divergence, that is <math id="S3.p4.m7" class="ltx_math_unparsed" alttext="\mathcal{D}_{\textrm{JS}}(p_{\theta}\mid\mid p^{*})" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>JS</mtext></msub><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0.0835em">∣</mo><mo lspace="0.0835em" rspace="0.167em">∣</mo><msup><mi>p</mi><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math> <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">goodfellow2014generative</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">nowozin2016f</span>)</cite> by observing that the Bayes-optimal classifier satisfies <math id="S3.p4.m8" class="ltx_Math" alttext="D^{*}(x)=\frac{p^{*}(x)}{p^{*}(x)+p^{\theta}(x)}" display="inline"><mrow><mrow><msup><mi>D</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>p</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mrow><msup><mi>p</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>p</mi><mi>θ</mi></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mrow></math> (assuming that prior probability of true data and fake data is balanced). Because we do not require an explicit density under the generative distribution and only require the ability to sample the distribution, this allows construction of imitation learning methods such as GAIL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ho2016generative</span>)</cite> which minimizes <math id="S3.p4.m9" class="ltx_math_unparsed" alttext="\mathcal{D}_{\textrm{JS}}(\rho^{\pi}(s,a)\mid\mid\rho^{*}(s,a))" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>JS</mtext></msub><mrow><mo stretchy="false">(</mo><msup><mi>ρ</mi><mi>π</mi></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.0835em">∣</mo><mo lspace="0.0835em" rspace="0.167em">∣</mo><msup><mi>ρ</mi><mo>*</mo></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, where the policy <math id="S3.p4.m10" class="ltx_Math" alttext="\pi" display="inline"><mi>π</mi></math> is rolled out in the environment starting from initial state distribution <math id="S3.p4.m11" class="ltx_Math" alttext="\rho_{0}" display="inline"><msub><mi>ρ</mi><mn>0</mn></msub></math> to generate samples from the state-action distribution <math id="S3.p4.m12" class="ltx_Math" alttext="\rho^{\pi}(s,a)" display="inline"><mrow><msup><mi>ρ</mi><mi>π</mi></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></math> and <math id="S3.p4.m13" class="ltx_Math" alttext="\rho^{*}(s,a)" display="inline"><mrow><msup><mi>ρ</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></math> is the target state-action distribution of the demonstrations.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Matching Expert Distributions for Autonomous Learning (MEDAL)</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">Several prior works demonstrate the ineffectiveness of standard RL methods in non-episodic settings <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">co2020ecological</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu20ingredients</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2021autonomous</span>)</cite>. Adding noise to actions, for example <math id="S4.p1.m1" class="ltx_Math" alttext="\epsilon" display="inline"><mi>ϵ</mi></math>-greedy in DQN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">mnih2015human</span>)</cite> or Gaussian noise in SAC <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">haarnoja2018soft</span>)</cite>), can be sufficient for exploration in episodic setting where every trial starts from a narrow initial state distribution. However, such an approach becomes ineffective in non-episodic settings because the same policy is expected to both solve the task and be sufficiently exploratory. As a result, a common solution in non-episodic autonomous RL settings is to learn another policy in addition to the forward policy <math id="S4.p1.m2" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math> that solves the task <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">han2015learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">eysenbach2017leave</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu20ingredients</span>)</cite>:
a backward policy <math id="S4.p1.m3" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> that targets a set of states to explore solving the task from. More precisely, the forward policy <math id="S4.p1.m4" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math> learns to solve the task from a state sampled from <math id="S4.p1.m5" class="ltx_Math" alttext="\rho^{b}" display="inline"><msup><mi>ρ</mi><mi>b</mi></msup></math>, the marginal state distribution of <math id="S4.p1.m6" class="ltx_Math" alttext="\pi^{b}" display="inline"><msup><mi>π</mi><mi>b</mi></msup></math>.
An appropriate <math id="S4.p1.m7" class="ltx_Math" alttext="\rho^{b}" display="inline"><msup><mi>ρ</mi><mi>b</mi></msup></math> can improve the efficiency of learning <math id="S4.p1.m8" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math> by creating an effective initial state distribution for it. What should the <math id="S4.p1.m9" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> optimize? We discuss this question in Section <a href="#S4.SS1" title="4.1 Finding Better Starting States ‣ 4 Matching Expert Distributions for Autonomous Learning (MEDAL) ‣ A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and a practical way to optimize the suggested objective in Section <a href="#S4.SS2" title="4.2 Resetting to Match the Expert State Distribution ‣ 4 Matching Expert Distributions for Autonomous Learning (MEDAL) ‣ A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. An overview of our proposed algorithm is given in Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:alg:summary</span>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="figures/tabletop_init_state_transfer.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="348" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of sampling initial states <math id="S4.F2.m5" class="ltx_Math" alttext="s_{0}" display="inline"><msub><mi>s</mi><mn>0</mn></msub></math> from the state distribution of the optimal policy <math id="S4.F2.m6" class="ltx_Math" alttext="\rho^{*}" display="inline"><msup><mi>ρ</mi><mo>*</mo></msup></math>, with sampling the initial state from the default distribution <math id="S4.F2.m7" class="ltx_Math" alttext="\rho_{0}" display="inline"><msub><mi>ρ</mi><mn>0</mn></msub></math> in the episodic setting. The episodic return is computed by initializing the agent at <math id="S4.F2.m8" class="ltx_Math" alttext="s_{0}\sim\rho_{0}" display="inline"><mrow><msub><mi>s</mi><mn>0</mn></msub><mo>∼</mo><msub><mi>ρ</mi><mn>0</mn></msub></mrow></math> in both the cases. The former improves both the sample efficiency and the performance of the final policy.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Finding Better Starting States</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">In episodic settings, <math id="S4.SS1.p1.m1" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math> always starts exploring from <math id="S4.SS1.p1.m2" class="ltx_Math" alttext="\rho_{0}" display="inline"><msub><mi>ρ</mi><mn>0</mn></msub></math>, which is the same distribution from which it will be evaluated. A natural objective for <math id="S4.SS1.p1.m3" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> then is to minimize the distance between <math id="S4.SS1.p1.m4" class="ltx_Math" alttext="\rho^{b}" display="inline"><msup><mi>ρ</mi><mi>b</mi></msup></math> and <math id="S4.SS1.p1.m5" class="ltx_Math" alttext="\rho_{0}" display="inline"><msub><mi>ρ</mi><mn>0</mn></msub></math>. And indeed, prior works have proposed this approach <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">han2015learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">eysenbach2017leave</span>)</cite> by learning a backward controller to retrieve the initial state distribution <math id="S4.SS1.p1.m6" class="ltx_Math" alttext="\rho_{0}" display="inline"><msub><mi>ρ</mi><mn>0</mn></msub></math>. While the initial state distribution cannot be changed in the episodic setting, <math id="S4.SS1.p1.m7" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> does not have any restriction to match <math id="S4.SS1.p1.m8" class="ltx_Math" alttext="\rho_{0}" display="inline"><msub><mi>ρ</mi><mn>0</mn></msub></math> in the autonomous RL setting. Is there a better initial state distribution to efficiently learn <math id="S4.SS1.p1.m9" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math> from?</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">Interestingly, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">kakade2002approximately</span></cite> provide a theoretical discussion on how the initial state distribution affects the performance. The main idea is that learning an optimal policy often requires policy improvement at states that are unlikely to be visited. Creating a more uniform starting state distribution can accelerate policy improvement by encouraging policy improvement at those unlikely states. The formal statement can be found in <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kakade2002approximately</span>, Corollary 4.5)</cite>. Informally, the result states that the upper bound on the difference between the optimal performance and that of policy <math id="S4.SS1.p2.m1" class="ltx_Math" alttext="\pi" display="inline"><mi>π</mi></math> is proportional to <math id="S4.SS1.p2.m2" class="ltx_Math" alttext="\lVert\frac{\rho^{*}(s)}{\mu}\rVert_{\infty}" display="inline"><msub><mrow><mo fence="true" rspace="0em">∥</mo><mfrac><mrow><msup><mi>ρ</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mi>μ</mi></mfrac><mo fence="true" lspace="0em">∥</mo></mrow><mi mathvariant="normal">∞</mi></msub></math>, where <math id="S4.SS1.p2.m3" class="ltx_Math" alttext="\rho^{*}" display="inline"><msup><mi>ρ</mi><mo>*</mo></msup></math> is the state distribution of the optimal policy and <math id="S4.SS1.p2.m4" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math> is the initial state distribution. This suggests that an initial state distribution <math id="S4.SS1.p2.m5" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math> that is close to the optimal state distribution <math id="S4.SS1.p2.m6" class="ltx_Math" alttext="\rho^{*}" display="inline"><msup><mi>ρ</mi><mo>*</mo></msup></math> would enable efficient learning. Intuitively, some initial states in the optimal state distribution would simplify the exploration by being closer to high reward states, which can be bootstrapped upon to learn faster from the harder initial states. To empirically verify the theoretical results, we compare the learning speed of RL algorithm in the episodic setting on <span class="ltx_text ltx_font_italic">tabletop organization</span> (environment details in Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:experiments</span>) when starting from (a) the standard initial state distribution, that is <math id="S4.SS1.p2.m7" class="ltx_Math" alttext="s_{0}\sim\rho_{0}" display="inline"><mrow><msub><mi>s</mi><mn>0</mn></msub><mo>∼</mo><msub><mi>ρ</mi><mn>0</mn></msub></mrow></math>, versus (b) states sampled from the stationary distribution of the optimal policy, that is <math id="S4.SS1.p2.m8" class="ltx_Math" alttext="s_{0}\sim\rho^{*}(s)" display="inline"><mrow><msub><mi>s</mi><mn>0</mn></msub><mo>∼</mo><mrow><msup><mi>ρ</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>. We find in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Matching Expert Distributions for Autonomous Learning (MEDAL) ‣ A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> that the latter not only improves the learning speed, but also improves the performance by nearly 18%.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Resetting to Match the Expert State Distribution</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">The theoretical and empirical results in the previous section suggest that <math id="S4.SS2.p1.m1" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math> should attempt to solve the task from an initial state distribution that is close to <math id="S4.SS2.p1.m2" class="ltx_Math" alttext="\rho^{*}(s)" display="inline"><mrow><msup><mi>ρ</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math>, thus implying that <math id="S4.SS2.p1.m3" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> should try to match <math id="S4.SS2.p1.m4" class="ltx_Math" alttext="\rho^{*}(s)" display="inline"><mrow><msup><mi>ρ</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math>.
How do we match <math id="S4.SS2.p1.m5" class="ltx_Math" alttext="\rho^{b}" display="inline"><msup><mi>ρ</mi><mi>b</mi></msup></math> to <math id="S4.SS2.p1.m6" class="ltx_Math" alttext="\rho^{*}" display="inline"><msup><mi>ρ</mi><mo>*</mo></msup></math>? We will assume access to a small set of samples from <math id="S4.SS2.p1.m7" class="ltx_Math" alttext="\rho^{*}(s)" display="inline"><mrow><msup><mi>ρ</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math> in the form of demonstrations <math id="S4.SS2.p1.m8" class="ltx_Math" alttext="\mathcal{D}_{f}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>f</mi></msub></math>. Because we are limited to sampling <math id="S4.SS2.p1.m9" class="ltx_Math" alttext="\rho^{b}" display="inline"><msup><mi>ρ</mi><mi>b</mi></msup></math> and only have a fixed set of samples from <math id="S4.SS2.p1.m10" class="ltx_Math" alttext="\rho^{*}" display="inline"><msup><mi>ρ</mi><mo>*</mo></msup></math>, we consider the following optimization problem:</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1" class="ltx_Math" alttext="\min_{\pi_{b}}\max_{C}\mathbb{E}_{s\sim\rho^{*}}\big{[}\log C(s)\big{]}+%
\mathbb{E}_{s\sim\rho^{b}}\big{[}\log(1-C(s))\big{]}" display="block"><mrow><mrow><mrow><munder><mi>min</mi><msub><mi>π</mi><mi>b</mi></msub></munder><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mi>C</mi></munder><mo lspace="0.167em">⁡</mo><msub><mi>𝔼</mi><mrow><mi>s</mi><mo>∼</mo><msup><mi>ρ</mi><mo>*</mo></msup></mrow></msub></mrow></mrow><mo>⁢</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>C</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>𝔼</mi><mrow><mi>s</mi><mo>∼</mo><msup><mi>ρ</mi><mi>b</mi></msup></mrow></msub><mo>⁢</mo><mrow><mo maxsize="120%" minsize="120%">[</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>C</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math id="S4.SS2.p1.m11" class="ltx_Math" alttext="C:\mathcal{S}\mapsto[0,1]" display="inline"><mrow><mi>C</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo stretchy="false">↦</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></mrow></math> is a state-space classifier. This optimization is very much reminiscent of implicit distribution matching techniques used in <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">goodfellow2014generative</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">nowozin2016f</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ho2016generative</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ghasemipour2020divergence</span>)</cite> when only the samples are available and densities cannot be explicitly measured.
This can be interpreted as minimizing the Jensen-Shannon divergence <math id="S4.SS2.p1.m12" class="ltx_math_unparsed" alttext="\mathcal{D}_{\textrm{JS}}(\rho^{b}\mid\mid\rho^{*})" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>JS</mtext></msub><mrow><mo stretchy="false">(</mo><msup><mi>ρ</mi><mi>b</mi></msup><mo lspace="0em" rspace="0.0835em">∣</mo><mo lspace="0.0835em" rspace="0.167em">∣</mo><msup><mi>ρ</mi><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math>.
Following these prior works, <math id="S4.SS2.p1.m13" class="ltx_Math" alttext="C(s)" display="inline"><mrow><mi>C</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math> solves a binary classification where <math id="S4.SS2.p1.m14" class="ltx_Math" alttext="s\sim\rho^{*}" display="inline"><mrow><mi>s</mi><mo>∼</mo><msup><mi>ρ</mi><mo>*</mo></msup></mrow></math> has a label <math id="S4.SS2.p1.m15" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> and <math id="S4.SS2.p1.m16" class="ltx_Math" alttext="s\sim\rho^{b}" display="inline"><mrow><mi>s</mi><mo>∼</mo><msup><mi>ρ</mi><mi>b</mi></msup></mrow></math> has a label <math id="S4.SS2.p1.m17" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math>. Further, <math id="S4.SS2.p1.m18" class="ltx_Math" alttext="\pi_{b}" display="inline"><msub><mi>π</mi><mi>b</mi></msub></math> solves a RL problem to maximize <math id="S4.SS2.p1.m19" class="ltx_Math" alttext="{\mathbb{E}_{s\sim\rho^{b}}[r(s,a)]}" display="inline"><mrow><msub><mi>𝔼</mi><mrow><mi>s</mi><mo>∼</mo><msup><mi>ρ</mi><mi>b</mi></msup></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">[</mo><mrow><mi>r</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></math>, where the reward function <math id="S4.SS2.p1.m20" class="ltx_Math" alttext="r(s,a)=-\log(1-C(s))" display="inline"><mrow><mrow><mi>r</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo rspace="0.167em">−</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>C</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>. Assuming sufficiently expressive non-parametric function classes, <math id="S4.SS2.p1.m21" class="ltx_Math" alttext="(\rho^{*},0.5)" display="inline"><mrow><mo stretchy="false">(</mo><msup><mi>ρ</mi><mo>*</mo></msup><mo>,</mo><mn>0.5</mn><mo stretchy="false">)</mo></mrow></math> is a saddle point for the optimization in Equation <a href="#S4.E3" title="3 ‣ 4.2 Resetting to Match the Expert State Distribution ‣ 4 Matching Expert Distributions for Autonomous Learning (MEDAL) ‣ A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Relationship to Prior Imitation Learning Methods.</span> GAIL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ho2016generative</span>)</cite> proposes to match the state-action distribution <math id="S4.SS2.p2.m1" class="ltx_Math" alttext="\rho^{\pi}(s,a)" display="inline"><mrow><msup><mi>ρ</mi><mi>π</mi></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></math> to that of the expert <math id="S4.SS2.p2.m2" class="ltx_Math" alttext="\rho^{*}(s,a)" display="inline"><mrow><msup><mi>ρ</mi><mo>*</mo></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></math>, that is minimize <math id="S4.SS2.p2.m3" class="ltx_math_unparsed" alttext="\mathcal{D}_{\textrm{JS}}(\rho^{\pi}(s,a)\mid\mid\rho^{*}(s,a))" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>JS</mtext></msub><mrow><mo stretchy="false">(</mo><msup><mi>ρ</mi><mi>π</mi></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.0835em">∣</mo><mo lspace="0.0835em" rspace="0.167em">∣</mo><msup><mi>ρ</mi><mo>*</mo></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></math>. Prior works have considered the problem of imitation learning when state-only observations are available <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">torabi2019adversarial</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu2020off</span>)</cite> by minimizing <math id="S4.SS2.p2.m4" class="ltx_math_unparsed" alttext="\mathcal{D}_{f}(\rho^{\pi}(s,s^{\prime})\mid\mid\rho^{*}(s,s^{\prime}))" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>f</mi></msub><mrow><mo stretchy="false">(</mo><msup><mi>ρ</mi><mi>π</mi></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.0835em">∣</mo><mo lspace="0.0835em" rspace="0.167em">∣</mo><msup><mi>ρ</mi><mo>*</mo></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, where <math id="S4.SS2.p2.m5" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>-divergence enables generalized treatment of different discrepancy measures such KL-divergence of JS-divergence used in prior work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">nowozin2016f</span>)</cite>.
In contrast to these works, our work proposes to minimize <math id="S4.SS2.p2.m6" class="ltx_math_unparsed" alttext="\mathcal{D}_{\textrm{JS}}(\rho^{\pi}(s)\mid\mid\rho^{*}(s))" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>JS</mtext></msub><mrow><mo stretchy="false">(</mo><msup><mi>ρ</mi><mi>π</mi></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0.0835em">∣</mo><mo lspace="0.0835em" rspace="0.167em">∣</mo><msup><mi>ρ</mi><mo>*</mo></msup><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></math>. Furthermore, state distribution matching is only used for the backward policy in our algorithm, whereas the forward policy is maximizing return, as we summarize in the next section. Finally, unlike prior works, the motivation for matching the state distributions is to create an effective initial state distribution for the forward policy <math id="S4.SS2.p2.m7" class="ltx_Math" alttext="\pi_{f}" display="inline"><msub><mi>π</mi><mi>f</mi></msub></math>. Our experimental results in Section <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:gail</span> suggest that naively extending GAIL to non-episodic settings is not effective, validating the importance of these key differences.</p>
</div>
<span class="ltx_ERROR undefined">{algorithm}</span>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_caption">Matching Expert Distributions for Autonomous Learning (MEDAL)</span>



































</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Dec 28 14:55:51 2023 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
